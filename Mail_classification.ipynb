{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mail_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMtwXUhy8KI4Hxz6VEy+k8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Novae28/Machine_learning/blob/main/Mail_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvMBDPgZWNb1"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3BwdLhV5UdU",
        "outputId": "3e080640-d114-49ce-849d-d5c90a5bc568"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import brown\r\n",
        "nltk.download('brown')\r\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PENzPS0C5wPJ",
        "outputId": "a6bf1128-b9fc-4813-f8f6-8959a1d395e5"
      },
      "source": [
        "print(brown.categories())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oITd3PF-50JO"
      },
      "source": [
        "data=brown.sents(categories='news')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ1sGW2N7Bjj"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QgUf9zm6m-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b994a5-7e2f-47d8-b7d4-7648bfa347c7"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize #sent_tokenize breaks a paragraph into a sentence while a word_tokenize breaks a sentence into words"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wjHMLII7Ypk"
      },
      "source": [
        "#example\r\n",
        "sentence=\"Hello guys! How are you, hope you all are happy and want me to continue with this tutorial.\"\r\n",
        "words=word_tokenize(sentence)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgybGlggPgiv",
        "outputId": "a83781e2-78c3-4100-c367-31d8dc6533d5"
      },
      "source": [
        "words"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'guys',\n",
              " '!',\n",
              " 'How',\n",
              " 'are',\n",
              " 'you',\n",
              " ',',\n",
              " 'hope',\n",
              " 'you',\n",
              " 'all',\n",
              " 'are',\n",
              " 'happy',\n",
              " 'and',\n",
              " 'want',\n",
              " 'me',\n",
              " 'to',\n",
              " 'continue',\n",
              " 'with',\n",
              " 'this',\n",
              " 'tutorial',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgvH-XtZSKTo"
      },
      "source": [
        "#regex tokenization\r\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BDZByhISX_4",
        "outputId": "1a155fa4-f341-4887-8eee-1b3ca68f190f"
      },
      "source": [
        "#to contain emails as a single word in the tokenized set.\r\n",
        "tokenizer=RegexpTokenizer('[a-zA-Z1234567890@.]+')\r\n",
        "sent1=\"This is keyur19.kt@gmail.com\"\r\n",
        "words2=tokenizer.tokenize(sent1)\r\n",
        "words2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'keyur19.kt@gmail.com']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15CQgaEX8ejU"
      },
      "source": [
        "### Stopword removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNUi2wwe8mRL",
        "outputId": "bb18ab75-f5df-40c0-df04-621c974b8528"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aczfM_658r0_",
        "outputId": "894d1f01-766e-4bfa-fb5d-df42eed3ac87"
      },
      "source": [
        "k=set(stopwords.words(\"english\"))\r\n",
        "print(k)\r\n",
        "len(k)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'it', 'being', 'or', 'she', 'm', 'them', 'doing', 'isn', 'the', 'd', 'hadn', 'any', 'has', 'we', 'here', 'ours', 'yourselves', 'they', 'theirs', 'not', 'before', 'her', 'who', 'further', 'can', 'be', 'are', 'ain', 'don', 'that', 'off', 'out', 'my', 'our', 'was', 'haven', 'if', 'himself', \"didn't\", 'both', 'same', \"it's\", \"don't\", \"doesn't\", 'mightn', 'just', 'were', 'under', 'having', 'their', \"mustn't\", 'other', 'been', 'of', 'herself', 'couldn', 'but', 'myself', 'whom', 'while', 'once', 'which', 's', \"hadn't\", 'shouldn', 'there', 'o', 'am', 'so', 'again', 'shan', 'should', \"needn't\", \"you'd\", 'most', 'have', 'i', 'where', 'now', 'ourselves', 'ma', 'did', 'an', 'he', 'through', 'for', 'your', \"haven't\", \"weren't\", 'you', 'during', 'aren', 'into', \"couldn't\", 'doesn', 'all', 'does', 'as', 'above', 'y', 'than', 'after', 'yourself', 'such', \"shouldn't\", \"shan't\", 'mustn', 'needn', 'had', 'some', 't', 'up', 'because', 'very', \"hasn't\", \"that'll\", 'on', 'hasn', 'didn', 'over', 've', 'no', 'how', 'with', 'what', 'these', 'wouldn', 'me', 'yours', 'to', 'from', 'why', 'at', \"she's\", 'few', \"aren't\", 'below', 'will', \"wouldn't\", 'll', \"you've\", 'against', 'then', \"should've\", 're', 'by', 'only', 'themselves', 'a', 'about', 'won', 'wasn', 'those', 'itself', 'each', 'own', 'do', 'until', 'between', 'more', 'in', 'hers', 'and', \"you're\", 'its', 'weren', 'too', 'nor', 'his', 'is', \"mightn't\", \"won't\", \"wasn't\", 'when', \"you'll\", \"isn't\", 'him', 'this', 'down'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ds314nA8v-3"
      },
      "source": [
        "def remove_stopwords(text,stopwords): # removing stopwords from the text data we have\r\n",
        "  punctuations=['.',',','?',':',';','!','%','/','-']\r\n",
        "  useful_words=[word for word in text if word not in stopwords and word not in punctuations]\r\n",
        "  return useful_words"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqHV81uzNQU2",
        "outputId": "1b4b9824-58ca-493d-e94c-a643752c5bdf"
      },
      "source": [
        "#example\r\n",
        "clarified_text=remove_stopwords(words,k)\r\n",
        "clarified_text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'guys', 'How', 'hope', 'happy', 'want', 'continue', 'tutorial']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXttZLMWRRLb"
      },
      "source": [
        "## Steming and lemetization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LZoJEONYNaVz",
        "outputId": "e7f95694-e250-42ef-80d1-9c89aa0c5f5c"
      },
      "source": [
        "# This process in nothing but removal of the words in different verb forms but mean the same.\r\n",
        "# words as 'like','liked','likes' basically mean the same, therefore the word 'like' is enough to make sure there is some sense of that word.\r\n",
        "from nltk.stem import PorterStemmer # we also have SnowballStemmer\r\n",
        "ps=PorterStemmer()\r\n",
        "ps.stem(\"like\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'like'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13Duo_XtUXvB"
      },
      "source": [
        "## Count vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q7ajPPYWqDO"
      },
      "source": [
        "#example \r\n",
        "Corpus=[\"assume these words here\",\" will be quite a good start to\",\"see what could be done with\",\"the power of just a few lines of code.\",\"I personally recommend my favorite publication\",\"“Text Similarities : Estimate the degree of similarity between two texts” regarding text similarities which can be found right here on medium.com.\",\"If you wanted to dig deeper into the text processing procedures.\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8-OeOWRUE_n",
        "outputId": "a92f4132-1bcd-431d-d070-d656a40a01ea"
      },
      "source": [
        "# we convert a text into a vector, this is nothing but a count of word occurance againts the word itself\r\n",
        "# corpus here is nothing but the text we are gonna vectorize\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cv=CountVectorizer()#creating an object of the class\r\n",
        "vectorized_corpus=cv.fit_transform(Corpus)# only takes a text input and not a string input, it can be an array of strings as well.\r\n",
        "vectorized_corpus=vectorized_corpus.toarray()\r\n",
        "print(vectorized_corpus[0])#length of each component the vectorized_corpus is gonna be the length of the vocabulary set\r\n",
        "print(cv.vocabulary_)# prints word with its provided index, when we run a query for that index, we get the count for the occurance of the word"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 0]\n",
            "{'assume': 0, 'these': 41, 'words': 49, 'here': 16, 'will': 47, 'be': 1, 'quite': 30, 'good': 15, 'start': 37, 'to': 42, 'see': 34, 'what': 45, 'could': 6, 'done': 10, 'with': 48, 'the': 40, 'power': 26, 'of': 23, 'just': 19, 'few': 13, 'lines': 20, 'code': 4, 'personally': 25, 'recommend': 31, 'my': 22, 'favorite': 12, 'publication': 29, 'text': 38, 'similarities': 35, 'estimate': 11, 'degree': 8, 'similarity': 36, 'between': 2, 'two': 43, 'texts': 39, 'regarding': 32, 'which': 46, 'can': 3, 'found': 14, 'right': 33, 'on': 24, 'medium': 21, 'com': 5, 'if': 17, 'you': 50, 'wanted': 44, 'dig': 9, 'deeper': 7, 'into': 18, 'processing': 28, 'procedures': 27}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUpsMgUtUdWF",
        "outputId": "c3b37cfa-5ede-4e4d-8889-dd00639bfd39"
      },
      "source": [
        "#removal of stopwords from the vectorized set\r\n",
        "def mytokenizer(document):\r\n",
        "  words=tokenizer.tokenize(document.lower())\r\n",
        "  #remove stopwords\r\n",
        "  words=remove_stopwords(words,k)\r\n",
        "  print(words)\r\n",
        "  return k\r\n",
        "\r\n",
        "cv2=CountVectorizer(tokenizer=mytokenizer)\r\n",
        "vectorized_corpus2=cv2.fit_transform(Corpus).toarray()\r\n",
        "vectorized_corpus2"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['assume', 'these', 'words', 'here']\n",
            "1\n",
            "['assume', 'words']\n",
            "['will', 'be', 'quite', 'a', 'good', 'start', 'to']\n",
            "1\n",
            "['quite', 'good', 'start']\n",
            "['see', 'what', 'could', 'be', 'done', 'with']\n",
            "1\n",
            "['see', 'could', 'done']\n",
            "['the', 'power', 'of', 'just', 'a', 'few', 'lines', 'of', 'code.']\n",
            "1\n",
            "['power', 'lines', 'code.']\n",
            "['i', 'personally', 'recommend', 'my', 'favorite', 'publication']\n",
            "1\n",
            "['personally', 'recommend', 'favorite', 'publication']\n",
            "['text', 'similarities', 'estimate', 'the', 'degree', 'of', 'similarity', 'between', 'two', 'texts', 'regarding', 'text', 'similarities', 'which', 'can', 'be', 'found', 'right', 'here', 'on', 'medium.com.']\n",
            "1\n",
            "['text', 'similarities', 'estimate', 'degree', 'similarity', 'two', 'texts', 'regarding', 'text', 'similarities', 'found', 'right', 'medium.com.']\n",
            "['if', 'you', 'wanted', 'to', 'dig', 'deeper', 'into', 'the', 'text', 'processing', 'procedures.']\n",
            "1\n",
            "['wanted', 'dig', 'deeper', 'text', 'processing', 'procedures.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHd6P36bYFGv",
        "outputId": "2dc22475-3064-48d4-98f6-f472458a9d6b"
      },
      "source": [
        "print(cv2.vocabulary_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'it': 74, 'being': 21, 'or': 103, 'she': 116, 'm': 80, 'them': 132, 'doing': 38, 'isn': 72, 'the': 129, 'd': 30, 'hadn': 49, 'any': 11, 'has': 51, 'we': 151, 'here': 60, 'ours': 106, 'yourselves': 178, 'they': 137, 'theirs': 131, 'not': 95, 'before': 20, 'her': 59, 'who': 160, 'further': 47, 'can': 27, 'be': 17, 'are': 12, 'ain': 6, 'don': 39, 'that': 127, 'off': 99, 'out': 108, 'my': 89, 'our': 105, 'was': 148, 'haven': 55, 'if': 68, 'himself': 64, \"didn't\": 33, 'both': 24, 'same': 113, \"it's\": 75, \"don't\": 40, \"doesn't\": 37, 'mightn': 83, 'just': 78, 'were': 152, 'under': 143, 'having': 57, 'their': 130, \"mustn't\": 88, 'other': 104, 'been': 19, 'of': 98, 'herself': 62, 'couldn': 28, 'but': 25, 'myself': 90, 'whom': 161, 'while': 159, 'once': 101, 'which': 158, 's': 112, \"hadn't\": 50, 'shouldn': 120, 'there': 135, 'o': 97, 'am': 8, 'so': 122, 'again': 4, 'shan': 114, 'should': 118, \"needn't\": 92, \"you'd\": 171, 'most': 86, 'have': 54, 'i': 67, 'where': 157, 'now': 96, 'ourselves': 107, 'ma': 81, 'did': 31, 'an': 9, 'he': 58, 'through': 140, 'for': 45, 'your': 175, \"haven't\": 56, \"weren't\": 154, 'you': 170, 'during': 42, 'aren': 13, 'into': 70, \"couldn't\": 29, 'doesn': 36, 'all': 7, 'does': 35, 'as': 15, 'above': 2, 'y': 169, 'than': 126, 'after': 3, 'yourself': 177, 'such': 124, \"shouldn't\": 121, \"shan't\": 115, 'mustn': 87, 'needn': 91, 'had': 48, 'some': 123, 't': 125, 'up': 145, 'because': 18, 'very': 147, \"hasn't\": 53, \"that'll\": 128, 'on': 100, 'hasn': 52, 'didn': 32, 'over': 109, 've': 146, 'no': 93, 'how': 66, 'with': 164, 'what': 155, 'these': 136, 'wouldn': 167, 'me': 82, 'yours': 176, 'to': 141, 'from': 46, 'why': 162, 'at': 16, \"she's\": 117, 'few': 44, \"aren't\": 14, 'below': 22, 'will': 163, \"wouldn't\": 168, 'll': 79, \"you've\": 174, 'against': 5, 'then': 134, \"should've\": 119, 're': 111, 'by': 26, 'only': 102, 'themselves': 133, 'a': 0, 'about': 1, 'won': 165, 'wasn': 149, 'those': 139, 'itself': 77, 'each': 43, 'own': 110, 'do': 34, 'until': 144, 'between': 23, 'more': 85, 'in': 69, 'hers': 61, 'and': 10, \"you're\": 173, 'its': 76, 'weren': 153, 'too': 142, 'nor': 94, 'his': 65, 'is': 71, \"mightn't\": 84, \"won't\": 166, \"wasn't\": 150, 'when': 156, \"you'll\": 172, \"isn't\": 73, 'him': 63, 'this': 138, 'down': 41}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmyW4l22de0L"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Zb95ylZidUOU",
        "outputId": "e7caf8ce-ad57-492b-8aad-e18de67cabc3"
      },
      "source": [
        "#using UCI sms spam collection dataset\r\n",
        "# https://www.kaggle.com/uciml/sms-spam-collection-dataset/home?select=spam.csv\r\n",
        "data=pd.read_csv(\"/content/spam.csv\",engine='python',error_bad_lines=False,encoding='ISO-8859-1')\r\n",
        "data"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        v1  ... Unnamed: 4\n",
              "0      ham  ...        NaN\n",
              "1      ham  ...        NaN\n",
              "2     spam  ...        NaN\n",
              "3      ham  ...        NaN\n",
              "4      ham  ...        NaN\n",
              "...    ...  ...        ...\n",
              "5567  spam  ...        NaN\n",
              "5568   ham  ...        NaN\n",
              "5569   ham  ...        NaN\n",
              "5570   ham  ...        NaN\n",
              "5571   ham  ...        NaN\n",
              "\n",
              "[5572 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cSXAslQnSUp"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5EfcRoCn5GI",
        "outputId": "360deb5d-9cd0-411c-f2b0-ff1105068bdc"
      },
      "source": [
        "le=LabelEncoder()\r\n",
        "data=data.to_numpy()\r\n",
        "y=data[:,0]\r\n",
        "x=data[:,1]\r\n",
        "x.shape,y.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5572,), (5572,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBsU8L4QpIXv"
      },
      "source": [
        "tokenizer3=RegexpTokenizer('\\w+')\r\n",
        "sw=k #stopwords mentioned before this step is not necessary\r\n",
        "def getStemmerReview(review):\r\n",
        "  review=review.lower()\r\n",
        "  stopwrds=set(stopwords.words(\"english\"))\r\n",
        "  wrds=tokenizer3.tokenize(review)\r\n",
        "  #removing the stopwords\r\n",
        "  new_tokens=remove_stopwords(wrds,stopwrds)\r\n",
        "  # print(new_tokens)\r\n",
        "  #stemming\r\n",
        "  stemmed_tokens=[ps.stem(token) for token in new_tokens]\r\n",
        "  # print(1)\r\n",
        "  # print(stemmed_tokens)\r\n",
        "  return \" \".join(stemmed_tokens)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4KkR9Fvq_0C"
      },
      "source": [
        "def getStemmed_docs(document):\r\n",
        "  d=[]\r\n",
        "  for doc in document:\r\n",
        "    d.append(getStemmerReview(doc))\r\n",
        "  return d"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHYCYKnYrStL"
      },
      "source": [
        "stemmed_document=getStemmed_docs(x)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZRBsmqfrj9n",
        "outputId": "70e203bc-c676-44d2-b952-490d6d508b1f"
      },
      "source": [
        "stemmed_document[:10]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
              " 'ok lar joke wif u oni',\n",
              " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
              " 'u dun say earli hor u c alreadi say',\n",
              " 'nah think goe usf live around though',\n",
              " 'freemsg hey darl 3 week word back like fun still tb ok xxx std chg send å 1 50 rcv',\n",
              " 'even brother like speak treat like aid patent',\n",
              " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend callertun',\n",
              " 'winner valu network custom select receivea å 900 prize reward claim call 09061701461 claim code kl341 valid 12 hour',\n",
              " 'mobil 11 month u r entitl updat latest colour mobil camera free call mobil updat co free 08002986030']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxakbVGXrpFO"
      },
      "source": [
        "#creating vector\r\n",
        "vectorize_corpus3=cv.fit_transform(stemmed_document)\r\n",
        "x=vectorize_corpus3.todense()\r\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=0.3)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZqiNTHqs8eP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721c461c-4e11-4c7d-81a3-7d3ce2a2bf04"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "model=MultinomialNB()\r\n",
        "model.fit(x_train,y_train)\r\n",
        "model.score(x_test,y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.979066985645933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0SVrjBVuhU4"
      },
      "source": [
        "#for the actual usage of the model that we created\r\n",
        "def prepare_message(message):\r\n",
        "  d=getStemmed_docs(message)\r\n",
        "  #we have already fitted the data to cv\r\n",
        "  return cv.transform(d)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAMgmJL0vXui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f89172b1-88c3-4ef4-b843-0a57a21f6067"
      },
      "source": [
        "message=[\"\"\"Dear Learner,\r\n",
        "\r\n",
        "Thank you for taking the course with NPTEL!!\r\n",
        "Hope you enjoyed the journey with us.\r\n",
        "The results for this course have been published and we are closing this course now. \r\n",
        "You will still have access to the contents and assignments of this course, if you click on the course name from the \"Mycourses\" tab on swayam.gov.in.\r\n",
        "The discussion forum is being closed though and you cannot ask questions here.\r\n",
        "\r\n",
        "For any further queries please write to support@nptel.iitm.ac.in.\r\n",
        "\r\n",
        "Enrollments are still open for 500+ courses for the Jan 2021 Semester!!!\r\n",
        "Enrollment for First set of courses will be closed by Feb 1,2021\r\n",
        "Please Click here to see the Final course list for Jan 2021:\"\"\",\r\n",
        "\"\"\" Hey there!\r\n",
        "\r\n",
        "Last 1 Day left for you to enrol for Batch 8 of Web Development Guided Path!\r\n",
        "\r\n",
        "Link is open for Batch 8 till 9 AM of 22nd January!\r\n",
        "\r\n",
        "Join here: https://rzp.io/l/guidancebatch\r\n",
        "\r\n",
        "We have covered all information in the session. \r\n",
        "\r\n",
        "Did you miss the LIVE Q&A session on Web Development Path and Guided Structured Programme?\r\n",
        "\r\n",
        "Watch it here: https://youtu.be/1AbrQuPYFJI\r\n",
        "\r\n",
        "At Progate, we want to help students in not just learning new skills but also in building some projects!\r\n",
        "\r\n",
        "What do you get in the Structured Guided Path?\r\n",
        "\r\n",
        ">> A proper study plan for 90 days. (60 mins each day)\r\n",
        "\r\n",
        ">> Access to 15 programming languages for 3 months.  \r\n",
        "\r\n",
        ">> 3 project building sessions and implementation while skilling up. \r\n",
        "\r\n",
        ">> Community Support from Progate Team when you are stuck while learning. \r\n",
        "\r\n",
        ">> Exclusive invites for Webathons (Web Development Hackathons).\r\n",
        "\r\n",
        ">> Web Development Certification course on Progate for 3 months... and a LOT MORE.\r\n",
        "\r\n",
        "We do live project building sessions, kickoffs, meetups, guidance sessions, webathons and a lot more to help you with your journey to becoming an independent coder as a Plus member!  ✨\r\n",
        "\r\n",
        "IMPORTANT: Whatever your goal is, you’ll be learning on a platform created by developers, for developers so you will understand that Progate has been designed for you, to get started with your programming journey!\r\n",
        "\r\n",
        "Don't miss the chance and join today! \r\n",
        "\r\n",
        "\r\n",
        "Regards,\r\n",
        "\r\n",
        "Ankita Mishra\r\n",
        "\r\n",
        "India Community Manager\r\n",
        "\r\n",
        "Progate\"\"\"]\r\n",
        "messages=prepare_message(message)\r\n",
        "y_predict=model.predict(messages)\r\n",
        "print(y_predict)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham' 'ham']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}